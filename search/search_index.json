{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to DeepEmotions","text":"<p>Team: Magdalena Buszka, Bartosz Brzoza, Martyna Firgolska </p>"},{"location":"#run-project","title":"Run project","text":"<pre><code>kedro run\n</code></pre>"},{"location":"#overview","title":"Overview","text":"<p>The aim of this project is to classify emotions associated with comments. For training we used google's goemotions dataset. </p>"},{"location":"data_download/","title":"Data download","text":"<p>To download data use data_downloading pipeline by running</p> <pre><code>kedro run\u00a0--pipeline=data_downloading\n</code></pre> <p>Alternatively you can download data manually by following instructions at goemotions to get datasets <code>goemotions_1.csv</code>, <code>goemotions_2.csv</code> and <code>goemotions_3.csv</code>. To get final dataset concatenate them into one <code>data/01_raw/goemotions-full-raw.csv</code> file.</p>"},{"location":"data_download/#overview","title":"Overview","text":"<p>This pipeline downloads raw data from google storage, concatenates it into one csv and saves it at <code>data/01_raw/goemotions-full-raw.csv</code>.</p>"},{"location":"data_download/#pipeline-inputs","title":"Pipeline inputs","text":"<p>List of remote datasets defined in <code>conf/base/catalog.yml</code>. In our case : <code>[\"remote-full-dataset-1\", \"remote-full-dataset-2\", \"remote-full-dataset-3\"]</code></p>"},{"location":"data_download/#nodes","title":"Nodes:","text":""},{"location":"data_download/#src.deepemotions.pipelines.data_download.nodes.download_data","title":"<code>download_data(*remote_datasets)</code>","text":"<p>Concatenates all remote datasets into one csv.</p> <p>Parameters:</p> Name Type Description Default <code>remote_datasets</code> <code>list(pandas.CSVDataSet)) </code> <p>csv datasets defined in conf/base/catalog.yml</p> <code>()</code> <p>Returns:</p> Name Type Description <code>full_dataset</code> <code>pandasCSVDataSet</code> <p>concatenated dataset</p> Source code in <code>src/deepemotions/pipelines/data_download/nodes.py</code> <pre><code>def download_data(*remote_datasets) -&gt; None:\n\"\"\"\n    Concatenates all remote datasets into one csv.\n\n    Args:\n        remote_datasets (list(pandas.CSVDataSet)) : csv datasets defined in conf/base/catalog.yml\n    Returns: \n        full_dataset(pandasCSVDataSet):  concatenated dataset\n    \"\"\"\n    # Concatenate all the datasets\n    full_dataset = pd.concat(remote_datasets)\n    print(\"Downloaded full datset of lenght:\", len(full_dataset))\n\n    return full_dataset\n</code></pre>"},{"location":"data_preprocess/","title":"Data preprocess","text":"<p>Do preprocess data use data_preprocess pipeline by running</p> <pre><code>kedro run\u00a0--pipeline=data_preprocess\n</code></pre>"},{"location":"data_preprocess/#overview","title":"Overview","text":"<p>This pipeline preproceses downloaded dataset <code>data/01_raw/goemotions-full-raw.csv</code> and saves it in <code>data/02_intermediate/formated_full.csv</code></p>"},{"location":"data_preprocess/#pipeline-inputs","title":"Pipeline inputs","text":"<p><code>full-raw-dataset</code> : pandas.CSVDataSet stored in <code>data/01_raw/goemotions-full-raw.csv</code></p>"},{"location":"data_preprocess/#nodes","title":"Nodes:","text":"<p>This is a boilerplate pipeline 'data_preprocess' generated using Kedro 0.18.5</p>"},{"location":"data_preprocess/#src.deepemotions.pipelines.data_preprocess.nodes.clean_dataset","title":"<code>clean_dataset(filtered_dataset)</code>","text":"<p>Clean </p> <p>Parameters:</p> Name Type Description Default <code>filtered_dataset</code> <code>pandas.CSVDataSet</code> required <p>Returns:</p> Type Description <code>CSVDataSet</code> <p>cleaned dataset</p> Source code in <code>src/deepemotions/pipelines/data_preprocess/nodes.py</code> <pre><code>def clean_dataset(filtered_dataset):\n\"\"\"Clean \n\n    Args:\n        filtered_dataset (pandas.CSVDataSet): \n\n    Returns:\n        (CSVDataSet): cleaned dataset\n    \"\"\"\n    clean_dataset = filtered_dataset.drop(['example_very_unclear'], axis = \"columns\")\n    return clean_dataset\n</code></pre>"},{"location":"data_preprocess/#src.deepemotions.pipelines.data_preprocess.nodes.drop_discordant_annotations","title":"<code>drop_discordant_annotations(grouped_dataset)</code>","text":"<p>Drops annotations given by just one rater.</p> <p>Parameters:</p> Name Type Description Default <code>grouped_dataset</code> <code>pandas.CSVDataSet</code> <p>dataset grouped by comment id and text</p> required <p>Returns:</p> Type Description <code>pandas.CSVDataSet</code> <p>filtered dataset</p> Source code in <code>src/deepemotions/pipelines/data_preprocess/nodes.py</code> <pre><code>def drop_discordant_annotations(grouped_dataset):\n\"\"\"Drops annotations given by just one rater.\n\n    Args:\n        grouped_dataset (pandas.CSVDataSet): dataset grouped by comment id and text\n\n    Returns:\n        (pandas.CSVDataSet): filtered dataset\n    \"\"\"\n    filtered_dataset = grouped_dataset.sum()\n    filtered_dataset = filtered_dataset.replace(to_replace = 1, value = 0).replace(\n        to_replace = [\n            k for k in range(2, filtered_dataset.max(axis = None) + 1 )],\n              value = 1)\n    filtered_dataset = filtered_dataset.loc[~(filtered_dataset==0).all(axis=1)]\n    filtered_dataset = filtered_dataset.reset_index()\n    return filtered_dataset \n</code></pre>"},{"location":"data_preprocess/#src.deepemotions.pipelines.data_preprocess.nodes.group_dataset","title":"<code>group_dataset(narrowed_dateset)</code>","text":"<p>Groups data in dataset by comment id.</p> <p>Parameters:</p> Name Type Description Default <code>narrowed_dateset</code> <code>pandas.CSVDataSet</code> <p>dataset with only relevant columns</p> required <p>Returns:</p> Type Description <code>CSVDataSet</code> <p>grouped dataset</p> Source code in <code>src/deepemotions/pipelines/data_preprocess/nodes.py</code> <pre><code>def group_dataset(narrowed_dateset):\n\"\"\"Groups data in dataset by comment id.\n\n    Args:\n        narrowed_dateset (pandas.CSVDataSet): dataset with only relevant columns\n\n    Returns:\n        (CSVDataSet): grouped dataset\n    \"\"\"\n    grouped_dataset = narrowed_dateset.groupby([\"id\", \"text\"])\n    return grouped_dataset \n</code></pre>"},{"location":"data_preprocess/#src.deepemotions.pipelines.data_preprocess.nodes.narrow_dataset","title":"<code>narrow_dataset(dataset)</code>","text":"<p>Drops unnecesary columns from dataset.  Remaining columns are: 'text', 'id', 'example_very_unclear' and columns representing emotion categories.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>pandas.CSVDataSet</code> <p>Dataset to be processed</p> required <p>Returns:</p> Type Description <code>pandas.CSVDataSet</code> <p>narrowed dataset</p> Source code in <code>src/deepemotions/pipelines/data_preprocess/nodes.py</code> <pre><code>def narrow_dataset(dataset):\n\"\"\"Drops unnecesary columns from dataset. \n    Remaining columns are: 'text', 'id', 'example_very_unclear' and columns representing emotion categories.\n\n    Args:\n        dataset (pandas.CSVDataSet): Dataset to be processed\n\n    Returns:\n        (pandas.CSVDataSet): narrowed dataset\n    \"\"\"\n    unnecesary_columns=[\"author\", \"subreddit\", \"link_id\",\n                        \"parent_id\", \"created_utc\", \"rater_id\"]\n    narrowed_dataset = dataset.drop(unnecesary_columns,\n                                    axis = \"columns\")\n    return narrowed_dataset\n</code></pre>"},{"location":"instalation/","title":"Installation","text":"<p>Follow this instruction install project and setup environment.</p>"},{"location":"instalation/#1-clone-the-repository","title":"1) Clone the repository","text":"<pre><code>git clone https://github.com/nerkulec/deepemotions.git\n</code></pre> <p>or </p> <pre><code>git clone git@github.com:nerkulec/deepemotions.git\n</code></pre>"},{"location":"instalation/#2-setup-environment","title":"2) Setup environment","text":"<p>To install this project make sure you have conda installed. After that simply open terminal, go to project folder and type:  </p> <pre><code>conda env create --file conda.yml\nconda activate deepemotions\npip install -r requirements.txt\n</code></pre>"},{"location":"model_download/","title":"Model download","text":"<p>To download model use model_downloading pipeline by running</p> <pre><code>kedro run\u00a0--pipeline=model_download\n</code></pre>"},{"location":"model_download/#overview","title":"Overview","text":"<p>This pipeline downloads pre train bert model.</p>"},{"location":"model_download/#pipeline-inputs","title":"Pipeline inputs","text":"<p>model_name(string) defined in <code>conf.base.parameters.model_download.yml</code></p>"},{"location":"model_download/#nodes","title":"Nodes:","text":""},{"location":"model_download/#src.deepemotions.pipelines.model_download.nodes.load_model","title":"<code>load_model(model_name, num_labels)</code>","text":"<p>Load Bert model</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>string</code> <p>Name of bert model to load</p> required <code>num_labels</code> <code>int</code> <p>Number of classess</p> required <p>Returns:</p> Type Description <code>BertModel</code> <p>loaded pretrained model</p> Source code in <code>src/deepemotions/pipelines/model_download/nodes.py</code> <pre><code>def load_model(model_name, num_labels):\n\"\"\"Load Bert model\n\n  Args:\n      model_name (string): Name of bert model to load\n      num_labels (int): Number of classess\n\n  Returns:\n      (BertModel): loaded pretrained model\n  \"\"\"\n  model = BertModel.from_pretrained(model_name, num_labels=num_labels)\n\n  return model\n</code></pre>"},{"location":"model_download/#src.deepemotions.pipelines.model_download.nodes.load_tokenizer","title":"<code>load_tokenizer(model_name)</code>","text":"<p>Loads Bert Tokenizer</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>string</code> <p>Name of bert tokenizer to be used</p> required <p>Returns:</p> Type Description <code>BertTokenizer</code> <p>tokenizer</p> Source code in <code>src/deepemotions/pipelines/model_download/nodes.py</code> <pre><code>def load_tokenizer(model_name):\n\"\"\"Loads Bert Tokenizer\n\n  Args:\n      model_name (string): Name of bert tokenizer to be used\n\n  Returns:\n      (BertTokenizer): tokenizer\n  \"\"\"\n  tokenizer = BertTokenizer.from_pretrained(model_name)\n\n  return tokenizer\n</code></pre>"},{"location":"model_train/","title":"Model training","text":"<p>To download model use model_downloading pipeline by running</p> <pre><code>kedro run\u00a0--pipeline=model_train\n</code></pre>"},{"location":"model_train/#overview","title":"Overview","text":"<p>This pipeline creates and trains model.</p>"},{"location":"model_train/#pipeline-inputs","title":"Pipeline inputs","text":"<p>clean-dataset, bert-tokenizer, bert-model defined in <code>conf.base.parameters.catalog.yml</code></p>"},{"location":"model_train/#nodes","title":"Nodes:","text":""},{"location":"model_train/#src.deepemotions.pipelines.model_train.nodes.get_datamodule","title":"<code>get_datamodule(df, tokenizer)</code>","text":"<p>Get EmotionDataModule</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas.CSVDataSet</code> <p>dataset</p> required <code>tokenizer</code> <code>BertTokenizer</code> <p>tokenizer to be used in datamodule</p> required <p>Returns:</p> Type Description <code>EmotionDataModule</code> <p>initialized Data Module</p> Source code in <code>src/deepemotions/pipelines/model_train/nodes.py</code> <pre><code>def get_datamodule(df, tokenizer):\n\"\"\"Get EmotionDataModule\n\n    Args:\n        df (pandas.CSVDataSet): dataset\n        tokenizer (BertTokenizer): tokenizer to be used in datamodule\n\n    Returns:\n        (EmotionDataModule): initialized Data Module\n    \"\"\"\n    return EmotionDataModule(df, tokenizer)\n</code></pre>"},{"location":"model_train/#src.deepemotions.pipelines.model_train.nodes.get_model","title":"<code>get_model(pretrained_bert)</code>","text":"<p>Gets EmotionModel</p> <p>Parameters:</p> Name Type Description Default <code>pretrained_bert</code> <code>BertModel</code> <p>Pre-trained Bert model</p> required <p>Returns:</p> Type Description <code>EmotionModel</code> <p>initalized Emotion Model</p> Source code in <code>src/deepemotions/pipelines/model_train/nodes.py</code> <pre><code>def get_model(pretrained_bert):\n\"\"\"Gets EmotionModel\n\n    Args:\n        pretrained_bert (BertModel): Pre-trained Bert model\n\n    Returns:\n        (EmotionModel): initalized Emotion Model\n    \"\"\"\n    return EmotionModel(pretrained_bert)\n</code></pre>"},{"location":"model_train/#src.deepemotions.pipelines.model_train.nodes.get_trainer","title":"<code>get_trainer()</code>","text":"<p>Get trainer</p> <p>Returns:</p> Type Description <code>pl.Trainer</code> <p>Trainer to be used to train model</p> Source code in <code>src/deepemotions/pipelines/model_train/nodes.py</code> <pre><code>def get_trainer():\n\"\"\"Get trainer\n\n    Returns:\n        (pl.Trainer): Trainer to be used to train model\n    \"\"\"\n    return pl.Trainer(\n        # gpus = 1,\n        max_epochs = 5,\n        # progress_bar_refresh_rate = 30\n    )\n</code></pre>"},{"location":"model_train/#src.deepemotions.pipelines.model_train.nodes.train_model","title":"<code>train_model(model, trainer, datamodule)</code>","text":"<p>Trains model</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>EmotionModel</code> <p>model to be trained</p> required <code>trainer</code> <code>pl.Trainer</code> <p>chosen trainer</p> required <code>datamodule</code> <code>EmotionDataModule</code> <p>datamodule with emotion dataset</p> required <p>Returns:</p> Name Type Description <code>EmotionModel</code> <p>trained model</p> Source code in <code>src/deepemotions/pipelines/model_train/nodes.py</code> <pre><code>def train_model(model, trainer, datamodule):\n\"\"\"Trains model\n\n    Args:\n        model (EmotionModel): model to be trained\n        trainer (pl.Trainer): chosen trainer\n        datamodule (EmotionDataModule): datamodule with emotion dataset\n\n    Returns:\n        EmotionModel: trained model\n    \"\"\"\n    trainer.fit(model, datamodule = datamodule)\n    return model\n</code></pre>"},{"location":"model_train/#dataset-and-datamodule","title":"Dataset and datamodule","text":""},{"location":"model_train/#src.deepemotions.pipelines.model_train.data.EmotionDataModule","title":"<code>EmotionDataModule</code>","text":"<p>         Bases: <code>pl.LightningDataModule</code></p> <p>EmotionDataModule class. Inherits from pythorch LightningDataModule</p> Source code in <code>src/deepemotions/pipelines/model_train/data.py</code> <pre><code>class EmotionDataModule(pl.LightningDataModule):\n\"\"\"EmotionDataModule class.\n    Inherits from pythorch LightningDataModule\n    \"\"\"\n    def __init__(self, df, tokenizer, batch_size = 32):\n\"\"\"Initializes data module.\n        Splits dataset into train/test/val.\n\n        Args:\n            df (Dataset): dataset \n            tokenizer (BertTokenizer): tokenizer\n            batch_size (int, optional): Size of batch. Defaults to 32.\n        \"\"\"\n        super().__init__()\n\n        train_df, val_df = train_test_split(df, test_size = 0.3, random_state = 42)\n        val_df, test_df = train_test_split(val_df, test_size = 0.5, random_state = 42)\n        self.train_df = train_df\n        self.val_df = val_df\n        self.test_df = test_df\n        self.batch_size = batch_size\n        self.tokenizer = tokenizer\n\n    def setup(self, stage = None):\n\"\"\"Performs setup\"\"\"\n        self.train_dataset = EmotionDataset(self.train_df, self.tokenizer)\n        self.val_dataset = EmotionDataset(self.val_df, self.tokenizer)\n        self.test_dataset = EmotionDataset(self.test_df, self.tokenizer)\n\n    def train_dataloader(self):\n\"\"\"Dataloader for training\"\"\"\n        return torch.utils.data.DataLoader(\n            self.train_dataset,\n            batch_size = self.batch_size,\n            shuffle = True,\n            num_workers = 4\n        )\n\n    def val_dataloader(self):\n\"\"\"Dataloader for validation\"\"\"\n        return torch.utils.data.DataLoader(\n            self.val_dataset,\n            batch_size = self.batch_size,\n            shuffle = False,\n            num_workers = 4\n        )\n\n    def test_dataloader(self):\n\"\"\"Dataloader for testing\"\"\"\n        return torch.utils.data.DataLoader(\n            self.val_dataset,\n            batch_size = self.batch_size,\n            shuffle = False,\n            num_workers = 4\n        )\n</code></pre>"},{"location":"model_train/#src.deepemotions.pipelines.model_train.data.EmotionDataModule.__init__","title":"<code>__init__(df, tokenizer, batch_size=32)</code>","text":"<p>Initializes data module. Splits dataset into train/test/val.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Dataset</code> <p>dataset </p> required <code>tokenizer</code> <code>BertTokenizer</code> <p>tokenizer</p> required <code>batch_size</code> <code>int</code> <p>Size of batch. Defaults to 32.</p> <code>32</code> Source code in <code>src/deepemotions/pipelines/model_train/data.py</code> <pre><code>def __init__(self, df, tokenizer, batch_size = 32):\n\"\"\"Initializes data module.\n    Splits dataset into train/test/val.\n\n    Args:\n        df (Dataset): dataset \n        tokenizer (BertTokenizer): tokenizer\n        batch_size (int, optional): Size of batch. Defaults to 32.\n    \"\"\"\n    super().__init__()\n\n    train_df, val_df = train_test_split(df, test_size = 0.3, random_state = 42)\n    val_df, test_df = train_test_split(val_df, test_size = 0.5, random_state = 42)\n    self.train_df = train_df\n    self.val_df = val_df\n    self.test_df = test_df\n    self.batch_size = batch_size\n    self.tokenizer = tokenizer\n</code></pre>"},{"location":"model_train/#src.deepemotions.pipelines.model_train.data.EmotionDataModule.setup","title":"<code>setup(stage=None)</code>","text":"<p>Performs setup</p> Source code in <code>src/deepemotions/pipelines/model_train/data.py</code> <pre><code>def setup(self, stage = None):\n\"\"\"Performs setup\"\"\"\n    self.train_dataset = EmotionDataset(self.train_df, self.tokenizer)\n    self.val_dataset = EmotionDataset(self.val_df, self.tokenizer)\n    self.test_dataset = EmotionDataset(self.test_df, self.tokenizer)\n</code></pre>"},{"location":"model_train/#src.deepemotions.pipelines.model_train.data.EmotionDataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Dataloader for testing</p> Source code in <code>src/deepemotions/pipelines/model_train/data.py</code> <pre><code>def test_dataloader(self):\n\"\"\"Dataloader for testing\"\"\"\n    return torch.utils.data.DataLoader(\n        self.val_dataset,\n        batch_size = self.batch_size,\n        shuffle = False,\n        num_workers = 4\n    )\n</code></pre>"},{"location":"model_train/#src.deepemotions.pipelines.model_train.data.EmotionDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Dataloader for training</p> Source code in <code>src/deepemotions/pipelines/model_train/data.py</code> <pre><code>def train_dataloader(self):\n\"\"\"Dataloader for training\"\"\"\n    return torch.utils.data.DataLoader(\n        self.train_dataset,\n        batch_size = self.batch_size,\n        shuffle = True,\n        num_workers = 4\n    )\n</code></pre>"},{"location":"model_train/#src.deepemotions.pipelines.model_train.data.EmotionDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Dataloader for validation</p> Source code in <code>src/deepemotions/pipelines/model_train/data.py</code> <pre><code>def val_dataloader(self):\n\"\"\"Dataloader for validation\"\"\"\n    return torch.utils.data.DataLoader(\n        self.val_dataset,\n        batch_size = self.batch_size,\n        shuffle = False,\n        num_workers = 4\n    )\n</code></pre>"},{"location":"model_train/#src.deepemotions.pipelines.model_train.data.EmotionDataset","title":"<code>EmotionDataset</code>","text":"<p>         Bases: <code>torch.utils.data.Dataset</code></p> <p>Emotion Dataset class. Inherits from pythorch Dataset</p> Source code in <code>src/deepemotions/pipelines/model_train/data.py</code> <pre><code>class EmotionDataset(torch.utils.data.Dataset):\n\"\"\"\n    Emotion Dataset class.\n    Inherits from pythorch Dataset\n    \"\"\"\n    def __init__(self, df, tokenizer):\n        text = df['text'].values\n        # self.X = text\n        self.X = tokenizer(\n            text.tolist(),\n            padding = True,\n            truncation = True,\n            max_length = 512,\n            return_tensors = 'pt'\n        )\n        emotions = df.columns[2:]\n        self.y = torch.tensor(df[emotions].values, dtype = torch.float32)\n\n    def __getitem__(self, idx):\n        return {\n            'input_ids': self.X['input_ids'][idx],\n            'attention_mask': self.X['attention_mask'][idx],\n            'token_type_ids': self.X['token_type_ids'][idx],\n        }, self.y[idx]\n\n    def __len__(self):\n        return len(self.X)\n</code></pre>"},{"location":"model_train/#model","title":"Model","text":""},{"location":"model_train/#src.deepemotions.pipelines.model_train.model.EmotionModel","title":"<code>EmotionModel</code>","text":"<p>         Bases: <code>pl.LightningModule</code></p> <p>EmotionModel class. Inherits from pytorch LightningModule.</p> Source code in <code>src/deepemotions/pipelines/model_train/model.py</code> <pre><code>class EmotionModel(pl.LightningModule):\n\"\"\"EmotionModel class. Inherits from pytorch LightningModule.\"\"\"\n    def __init__(self, pretrained_bert):\n\"\"\"Initializes model\n\n        Args:\n            pretrained_bert (BertModel): pre-trained bert model\n        \"\"\"\n        super().__init__()\n        self.bert = pretrained_bert\n        self.dropout = torch.nn.Dropout(0.3)\n        self.out = torch.nn.Linear(768, 28)\n\n    def forward(self, x):\n        output = self.bert(**x)\n        output = self.dropout(output.pooler_output)\n        output = self.out(output)\n        return torch.sigmoid(output)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = torch.nn.functional.binary_cross_entropy(y_hat, y)\n        self.log('train_loss', loss, on_step = True, on_epoch = True, prog_bar = True, logger = True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = torch.nn.functional.binary_cross_entropy(y_hat, y)\n        self.log('val_loss', loss, on_step = True, on_epoch = True, prog_bar = True, logger = True)\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = torch.nn.functional.binary_cross_entropy(y_hat, y)\n        self.log('test_loss', loss, on_step = True, on_epoch = True, prog_bar = True, logger = True)\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters())\n\n    def predict(self, text):\n        self.eval()\n        with torch.no_grad():\n            return self(text)\n</code></pre>"},{"location":"model_train/#src.deepemotions.pipelines.model_train.model.EmotionModel.__init__","title":"<code>__init__(pretrained_bert)</code>","text":"<p>Initializes model</p> <p>Parameters:</p> Name Type Description Default <code>pretrained_bert</code> <code>BertModel</code> <p>pre-trained bert model</p> required Source code in <code>src/deepemotions/pipelines/model_train/model.py</code> <pre><code>def __init__(self, pretrained_bert):\n\"\"\"Initializes model\n\n    Args:\n        pretrained_bert (BertModel): pre-trained bert model\n    \"\"\"\n    super().__init__()\n    self.bert = pretrained_bert\n    self.dropout = torch.nn.Dropout(0.3)\n    self.out = torch.nn.Linear(768, 28)\n</code></pre>"}]}